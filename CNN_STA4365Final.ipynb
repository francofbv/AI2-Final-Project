{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA4365 - Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading + Preprocessing\n",
    "df = pd.read_csv('train.csv')\n",
    "df = df.fillna(-9e10) # Filling NaN values with large negative numbers came at the advice of other competition participants; appears to have worked well\n",
    "# Low NaN counts in the feature columns used (~200 out of 5 million rows) makes the impact negligible nonetheless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes\n",
    "\n",
    "# Converting data to PyTorch tensors + compatibility with CNN\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, targets, window_size):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.window_size = window_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data.iloc[idx:idx+self.window_size].values.T\n",
    "        y = self.targets.iloc[idx + self.window_size]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "\n",
    "# CNN - see report for details on model design\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels=9, window_size=30):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.flattened_size = self._get_flattened_size(input_channels, window_size) # See function comment\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "    def _get_flattened_size(self, input_channels, window_size): # This function is just to get around computing the flattened size for different window sizes\n",
    "        x = torch.zeros(1, input_channels, window_size)\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        return x.view(1, -1).shape[1]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# Model training - forward / backprop\n",
    "\n",
    "def train_model(model, train_loader, num_epochs, optimizer, criterion, device):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Calculating MAE\n",
    "\n",
    "def evaluate_mae(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            outputs = model(x_batch).squeeze()\n",
    "            y_preds.extend(outputs.cpu().numpy())\n",
    "            y_trues.extend(y_batch.cpu().numpy())\n",
    "    mae = mean_absolute_error(y_trues, y_preds)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline for all 200 companies\n",
    "\n",
    "time_steps = 30\n",
    "learning_rate = 0.001\n",
    "num_epochs = 6\n",
    "\n",
    "mae_results = []\n",
    "for i in range(200):\n",
    "    if(i == 78):\n",
    "        continue # Skipping Company 79 due to issues with dataset; could not identify root cause ahead of the report\n",
    "\n",
    "    # Data loading, preprocessing for each company\n",
    "    df_comp = df[df[\"stock_id\"] == i]\n",
    "    X = df_comp[['seconds_in_bucket', \"imbalance_size\", \"imbalance_buy_sell_flag\", \"reference_price\", \"matched_size\", \"bid_price\", \"bid_size\", \"ask_price\", \"ask_size\", \"wap\"]]\n",
    "    y = df_comp[\"target\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False) # shuffle = false to maintain order in time series dataset (i.e. training set is the first 80% of data)\n",
    "    train_dataset = TimeSeriesDataset(X_train, y_train, time_steps)\n",
    "    test_dataset = TimeSeriesDataset(X_test, y_test, time_steps)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False) # See above comment on shuffle\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    # Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CNN(input_channels=10, window_size=30).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.L1Loss()\n",
    "    # Train model\n",
    "    train_model(model, train_loader, num_epochs, optimizer, criterion, device)\n",
    "    # Get results\n",
    "    mae = evaluate_mae(model, test_loader)\n",
    "    mae_results.append(mae)\n",
    "    print(\"Mean Absolute Error for CNN, Company #\", i, \":\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "\n",
    "print(\"Median MAE for all companies:\", np.median(mae_results))\n",
    "print(\"Best MAE:\", np.min(mae_results))\n",
    "\n",
    "# Boxplot\n",
    "mae_resultsLog = np.log(mae_results) # Log transform to make the boxplot readable\n",
    "plt.boxplot(mae_resultsLog)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
